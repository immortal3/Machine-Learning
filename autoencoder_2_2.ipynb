{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libs\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Input, Dense, GaussianNoise,Lambda\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for reproducing reslut\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M: 16 k: 4\n"
     ]
    }
   ],
   "source": [
    "# defining parameters\n",
    "M = 16\n",
    "k = np.log2(M)\n",
    "k = int(k)\n",
    "print ('M:',M,'k:',k)\n",
    "R = 1\n",
    "n_channel = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generating data of size N\n",
    "N = 10000\n",
    "label = np.random.randint(M,size=N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating one hot encoded vectors\n",
    "data = []\n",
    "for i in label:\n",
    "    temp = np.zeros(M)\n",
    "    temp[i] = 1\n",
    "    data.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 16)\n"
     ]
    }
   ],
   "source": [
    "data = np.array(data)\n",
    "print (data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "4 [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "13 [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "3 [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "9 [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "7 [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "12 [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "15 [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "12 [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "temp_check = [17,23,45,67,89,96,72,250,350]\n",
    "for i in temp_check:\n",
    "    print(label[i],data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def antirectifier(x):\n",
    "    y = x/K.l2_normalize(x,axis=0)\n",
    "    return y\n",
    "def antirectifier_output_shape(input_shape):\n",
    "    return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print (int(k/R))\n",
    "input_signal = Input(shape=(M,))\n",
    "encoded = Dense(M, activation='relu')(input_signal)\n",
    "encoded1 = Dense(n_channel, activation='linear')(encoded)\n",
    "encoded2 = Lambda(lambda x: (np.sqrt(n_channel)*x)/K.l2_normalize(x,axis=0))(encoded1)\n",
    "#encoded2 = BatchNormalization()(encoded1)\n",
    "EbNo_train =  7 # 5.01187 #  coverted 7 db of EbNo\n",
    "encoded3 = GaussianNoise(np.sqrt(1/(2*R*EbNo_train)))(encoded2)\n",
    "\n",
    "decoded = Dense(M, activation='relu')(encoded3)\n",
    "decoded1 = Dense(M, activation='softmax')(decoded)\n",
    "autoencoder = Model(input_signal, decoded1)\n",
    "adam = Adam(lr=0.001)\n",
    "autoencoder.compile(optimizer=adam, loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 7)                 119       \n",
      "_________________________________________________________________\n",
      "lambda_4 (Lambda)            (None, 7)                 0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_4 (GaussianNo (None, 7)                 0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 16)                128       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 16)                272       \n",
      "=================================================================\n",
      "Total params: 791\n",
      "Trainable params: 791\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print (autoencoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_val = 1500\n",
    "val_label = np.random.randint(M,size=N_val)\n",
    "val_data = []\n",
    "for i in val_label:\n",
    "    temp = np.zeros(M)\n",
    "    temp[i] = 1\n",
    "    val_data.append(temp)\n",
    "val_data = np.array(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 1500 samples\n",
      "Epoch 1/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7748 - val_loss: 2.7791\n",
      "Epoch 2/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7779 - val_loss: 2.7813\n",
      "Epoch 3/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7783 - val_loss: 2.7797\n",
      "Epoch 4/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7764 - val_loss: 2.7715\n",
      "Epoch 5/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7723 - val_loss: 2.7697\n",
      "Epoch 6/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7709 - val_loss: 2.7709\n",
      "Epoch 7/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7708 - val_loss: 2.7706\n",
      "Epoch 8/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7708 - val_loss: 2.7703\n",
      "Epoch 9/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7706 - val_loss: 2.7710\n",
      "Epoch 10/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7699 - val_loss: 2.7714\n",
      "Epoch 11/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7710 - val_loss: 2.7699\n",
      "Epoch 12/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7699 - val_loss: 2.7696\n",
      "Epoch 13/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7693 - val_loss: 2.7697\n",
      "Epoch 14/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7701 - val_loss: 2.7700\n",
      "Epoch 15/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7693 - val_loss: 2.7695\n",
      "Epoch 16/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7696 - val_loss: 2.7701\n",
      "Epoch 17/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7695 - val_loss: 2.7708\n",
      "Epoch 18/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7702 - val_loss: 2.7698\n",
      "Epoch 19/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7710 - val_loss: 2.7705\n",
      "Epoch 20/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7715 - val_loss: 2.7701\n",
      "Epoch 21/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7700 - val_loss: 2.7698\n",
      "Epoch 22/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7705 - val_loss: 2.7703\n",
      "Epoch 23/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7702 - val_loss: 2.7698\n",
      "Epoch 24/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7695 - val_loss: 2.7703\n",
      "Epoch 25/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7703 - val_loss: 2.7698\n",
      "Epoch 26/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7710 - val_loss: 2.7698\n",
      "Epoch 27/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7698 - val_loss: 2.7696\n",
      "Epoch 28/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7699 - val_loss: 2.7696\n",
      "Epoch 29/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7693 - val_loss: 2.7696\n",
      "Epoch 30/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7691 - val_loss: 2.7693\n",
      "Epoch 31/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7685 - val_loss: 2.7698\n",
      "Epoch 32/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7703 - val_loss: 2.7696\n",
      "Epoch 33/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7695 - val_loss: 2.7696\n",
      "Epoch 34/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7687 - val_loss: 2.7697\n",
      "Epoch 35/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7678 - val_loss: 2.7692\n",
      "Epoch 36/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7705 - val_loss: 2.7692\n",
      "Epoch 37/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7706 - val_loss: 2.7693\n",
      "Epoch 38/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7689 - val_loss: 2.7697\n",
      "Epoch 39/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7698 - val_loss: 2.7699\n",
      "Epoch 40/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7692 - val_loss: 2.7697\n",
      "Epoch 41/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7694 - val_loss: 2.7693\n",
      "Epoch 42/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7697 - val_loss: 2.7696\n",
      "Epoch 43/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7694 - val_loss: 2.7694\n",
      "Epoch 44/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7696 - val_loss: 2.7691\n",
      "Epoch 45/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7697 - val_loss: 2.7693\n",
      "Epoch 46/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7701 - val_loss: 2.7691\n",
      "Epoch 47/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7694 - val_loss: 2.7695\n",
      "Epoch 48/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7690 - val_loss: 2.7695\n",
      "Epoch 49/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7696 - val_loss: 2.7692\n",
      "Epoch 50/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7681 - val_loss: 2.7694\n",
      "Epoch 51/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7695 - val_loss: 2.7695\n",
      "Epoch 52/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7688 - val_loss: 2.7697\n",
      "Epoch 53/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7688 - val_loss: 2.7695\n",
      "Epoch 54/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7687 - val_loss: 2.7692\n",
      "Epoch 55/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7697 - val_loss: 2.7696\n",
      "Epoch 56/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7684 - val_loss: 2.7692\n",
      "Epoch 57/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7679 - val_loss: 2.7696\n",
      "Epoch 58/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7683 - val_loss: 2.7694\n",
      "Epoch 59/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7695 - val_loss: 2.7694\n",
      "Epoch 60/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7690 - val_loss: 2.7694\n",
      "Epoch 61/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7693 - val_loss: 2.7694\n",
      "Epoch 62/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7687 - val_loss: 2.7692\n",
      "Epoch 63/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7687 - val_loss: 2.7695\n",
      "Epoch 64/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7695 - val_loss: 2.7696\n",
      "Epoch 65/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7688 - val_loss: 2.7692\n",
      "Epoch 66/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7698 - val_loss: 2.7693\n",
      "Epoch 67/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7693 - val_loss: 2.7693\n",
      "Epoch 68/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7688 - val_loss: 2.7694\n",
      "Epoch 69/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7694 - val_loss: 2.7692\n",
      "Epoch 70/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7690 - val_loss: 2.7691\n",
      "Epoch 71/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7682 - val_loss: 2.7697\n",
      "Epoch 72/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7685 - val_loss: 2.7692\n",
      "Epoch 73/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7695 - val_loss: 2.7692\n",
      "Epoch 74/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7693 - val_loss: 2.7693\n",
      "Epoch 75/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7693 - val_loss: 2.7694\n",
      "Epoch 76/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7696 - val_loss: 2.7692\n",
      "Epoch 77/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7686 - val_loss: 2.7694\n",
      "Epoch 78/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7687 - val_loss: 2.7698\n",
      "Epoch 79/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7694 - val_loss: 2.7691\n",
      "Epoch 80/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7701 - val_loss: 2.7692\n",
      "Epoch 81/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7687 - val_loss: 2.7695\n",
      "Epoch 82/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7692 - val_loss: 2.7695\n",
      "Epoch 83/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7694 - val_loss: 2.7691\n",
      "Epoch 84/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7694 - val_loss: 2.7691\n",
      "Epoch 85/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s - loss: 2.7700 - val_loss: 2.7698\n",
      "Epoch 86/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7699 - val_loss: 2.7694\n",
      "Epoch 87/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7704 - val_loss: 2.7692\n",
      "Epoch 88/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7689 - val_loss: 2.7692\n",
      "Epoch 89/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7697 - val_loss: 2.7693\n",
      "Epoch 90/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7681 - val_loss: 2.7689\n",
      "Epoch 91/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7685 - val_loss: 2.7690\n",
      "Epoch 92/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7690 - val_loss: 2.7694\n",
      "Epoch 93/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7701 - val_loss: 2.7691\n",
      "Epoch 94/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7697 - val_loss: 2.7692\n",
      "Epoch 95/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7683 - val_loss: 2.7694\n",
      "Epoch 96/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7692 - val_loss: 2.7695\n",
      "Epoch 97/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7685 - val_loss: 2.7692\n",
      "Epoch 98/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7691 - val_loss: 2.7690\n",
      "Epoch 99/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7684 - val_loss: 2.7692\n",
      "Epoch 100/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7691 - val_loss: 2.7693\n",
      "Epoch 101/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7693 - val_loss: 2.7692\n",
      "Epoch 102/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7695 - val_loss: 2.7694\n",
      "Epoch 103/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7694 - val_loss: 2.7690\n",
      "Epoch 104/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7682 - val_loss: 2.7695\n",
      "Epoch 105/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7684 - val_loss: 2.7692\n",
      "Epoch 106/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7694 - val_loss: 2.7693\n",
      "Epoch 107/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7690 - val_loss: 2.7695\n",
      "Epoch 108/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7698 - val_loss: 2.7694\n",
      "Epoch 109/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7694 - val_loss: 2.7690\n",
      "Epoch 110/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7694 - val_loss: 2.7692\n",
      "Epoch 111/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7688 - val_loss: 2.7691\n",
      "Epoch 112/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7683 - val_loss: 2.7694\n",
      "Epoch 113/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7685 - val_loss: 2.7692\n",
      "Epoch 114/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7695 - val_loss: 2.7692\n",
      "Epoch 115/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7686 - val_loss: 2.7693\n",
      "Epoch 116/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7697 - val_loss: 2.7696\n",
      "Epoch 117/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7685 - val_loss: 2.7695\n",
      "Epoch 118/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7688 - val_loss: 2.7695\n",
      "Epoch 119/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7693 - val_loss: 2.7696\n",
      "Epoch 120/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7695 - val_loss: 2.7692\n",
      "Epoch 121/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7693 - val_loss: 2.7691\n",
      "Epoch 122/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7685 - val_loss: 2.7691\n",
      "Epoch 123/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7684 - val_loss: 2.7696\n",
      "Epoch 124/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7693 - val_loss: 2.7695\n",
      "Epoch 125/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7686 - val_loss: 2.7690\n",
      "Epoch 126/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7694 - val_loss: 2.7691\n",
      "Epoch 127/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7690 - val_loss: 2.7692\n",
      "Epoch 128/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7690 - val_loss: 2.7691\n",
      "Epoch 129/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7696 - val_loss: 2.7689\n",
      "Epoch 130/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7682 - val_loss: 2.7691\n",
      "Epoch 131/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7682 - val_loss: 2.7694\n",
      "Epoch 132/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7691 - val_loss: 2.7693\n",
      "Epoch 133/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7689 - val_loss: 2.7692\n",
      "Epoch 134/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7683 - val_loss: 2.7693\n",
      "Epoch 135/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7698 - val_loss: 2.7694\n",
      "Epoch 136/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7687 - val_loss: 2.7693\n",
      "Epoch 137/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7682 - val_loss: 2.7692\n",
      "Epoch 138/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7692 - val_loss: 2.7692\n",
      "Epoch 139/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7688 - val_loss: 2.7691\n",
      "Epoch 140/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7691 - val_loss: 2.7694\n",
      "Epoch 141/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7691 - val_loss: 2.7693\n",
      "Epoch 142/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7684 - val_loss: 2.7687\n",
      "Epoch 143/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7692 - val_loss: 2.7689\n",
      "Epoch 144/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7687 - val_loss: 2.7690\n",
      "Epoch 145/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7699 - val_loss: 2.7686\n",
      "Epoch 146/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7689 - val_loss: 2.7694\n",
      "Epoch 147/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7684 - val_loss: 2.7693\n",
      "Epoch 148/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7699 - val_loss: 2.7690\n",
      "Epoch 149/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7699 - val_loss: 2.7692\n",
      "Epoch 150/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7683 - val_loss: 2.7691\n",
      "Epoch 151/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7685 - val_loss: 2.7694\n",
      "Epoch 152/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7697 - val_loss: 2.7694\n",
      "Epoch 153/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7700 - val_loss: 2.7688\n",
      "Epoch 154/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7688 - val_loss: 2.7693\n",
      "Epoch 155/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7675 - val_loss: 2.7687\n",
      "Epoch 156/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7691 - val_loss: 2.7689\n",
      "Epoch 157/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7693 - val_loss: 2.7698\n",
      "Epoch 158/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7690 - val_loss: 2.7695\n",
      "Epoch 159/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7690 - val_loss: 2.7695\n",
      "Epoch 160/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7683 - val_loss: 2.7689\n",
      "Epoch 161/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7695 - val_loss: 2.7693\n",
      "Epoch 162/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7687 - val_loss: 2.7691\n",
      "Epoch 163/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7692 - val_loss: 2.7698\n",
      "Epoch 164/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7692 - val_loss: 2.7691\n",
      "Epoch 165/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7698 - val_loss: 2.7688\n",
      "Epoch 166/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7704 - val_loss: 2.7691\n",
      "Epoch 167/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7685 - val_loss: 2.7692\n",
      "Epoch 168/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s - loss: 2.7691 - val_loss: 2.7693\n",
      "Epoch 169/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7702 - val_loss: 2.7691\n",
      "Epoch 170/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7693 - val_loss: 2.7692\n",
      "Epoch 171/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7691 - val_loss: 2.7695\n",
      "Epoch 172/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7687 - val_loss: 2.7687\n",
      "Epoch 173/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7685 - val_loss: 2.7693\n",
      "Epoch 174/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7693 - val_loss: 2.7696\n",
      "Epoch 175/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7682 - val_loss: 2.7697\n",
      "Epoch 176/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7684 - val_loss: 2.7694\n",
      "Epoch 177/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7703 - val_loss: 2.7692\n",
      "Epoch 178/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7697 - val_loss: 2.7691\n",
      "Epoch 179/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7677 - val_loss: 2.7696\n",
      "Epoch 180/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7682 - val_loss: 2.7697\n",
      "Epoch 181/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7702 - val_loss: 2.7691\n",
      "Epoch 182/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7696 - val_loss: 2.7694\n",
      "Epoch 183/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7689 - val_loss: 2.7692\n",
      "Epoch 184/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7687 - val_loss: 2.7692\n",
      "Epoch 185/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7690 - val_loss: 2.7692\n",
      "Epoch 186/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7700 - val_loss: 2.7694\n",
      "Epoch 187/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7695 - val_loss: 2.7692\n",
      "Epoch 188/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7696 - val_loss: 2.7696\n",
      "Epoch 189/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7688 - val_loss: 2.7694\n",
      "Epoch 190/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7693 - val_loss: 2.7698\n",
      "Epoch 191/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7681 - val_loss: 2.7694\n",
      "Epoch 192/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7688 - val_loss: 2.7693\n",
      "Epoch 193/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7700 - val_loss: 2.7689\n",
      "Epoch 194/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7691 - val_loss: 2.7693\n",
      "Epoch 195/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7697 - val_loss: 2.7695\n",
      "Epoch 196/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7682 - val_loss: 2.7699\n",
      "Epoch 197/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7693 - val_loss: 2.7693\n",
      "Epoch 198/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7685 - val_loss: 2.7697\n",
      "Epoch 199/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7685 - val_loss: 2.7694\n",
      "Epoch 200/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7694 - val_loss: 2.7692\n",
      "Epoch 201/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7691 - val_loss: 2.7692\n",
      "Epoch 202/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7688 - val_loss: 2.7694\n",
      "Epoch 203/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7688 - val_loss: 2.7695\n",
      "Epoch 204/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7698 - val_loss: 2.7694\n",
      "Epoch 205/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7693 - val_loss: 2.7692\n",
      "Epoch 206/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7686 - val_loss: 2.7699\n",
      "Epoch 207/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7691 - val_loss: 2.7693\n",
      "Epoch 208/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7687 - val_loss: 2.7689\n",
      "Epoch 209/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7687 - val_loss: 2.7694\n",
      "Epoch 210/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7694 - val_loss: 2.7696\n",
      "Epoch 211/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7696 - val_loss: 2.7692\n",
      "Epoch 212/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7692 - val_loss: 2.7695\n",
      "Epoch 213/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7691 - val_loss: 2.7694\n",
      "Epoch 214/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7689 - val_loss: 2.7689\n",
      "Epoch 215/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7690 - val_loss: 2.7694\n",
      "Epoch 216/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7694 - val_loss: 2.7694\n",
      "Epoch 217/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7696 - val_loss: 2.7690\n",
      "Epoch 218/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7683 - val_loss: 2.7694\n",
      "Epoch 219/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7694 - val_loss: 2.7694\n",
      "Epoch 220/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7696 - val_loss: 2.7693\n",
      "Epoch 221/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7691 - val_loss: 2.7690\n",
      "Epoch 222/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7685 - val_loss: 2.7692\n",
      "Epoch 223/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7689 - val_loss: 2.7691\n",
      "Epoch 224/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7687 - val_loss: 2.7692\n",
      "Epoch 225/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7684 - val_loss: 2.7692\n",
      "Epoch 226/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7683 - val_loss: 2.7691\n",
      "Epoch 227/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7682 - val_loss: 2.7692\n",
      "Epoch 228/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7689 - val_loss: 2.7692\n",
      "Epoch 229/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7689 - val_loss: 2.7695\n",
      "Epoch 230/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7692 - val_loss: 2.7694\n",
      "Epoch 231/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7692 - val_loss: 2.7695\n",
      "Epoch 232/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7697 - val_loss: 2.7693\n",
      "Epoch 233/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7689 - val_loss: 2.7696\n",
      "Epoch 234/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7683 - val_loss: 2.7693\n",
      "Epoch 235/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7690 - val_loss: 2.7691\n",
      "Epoch 236/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7693 - val_loss: 2.7694\n",
      "Epoch 237/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7704 - val_loss: 2.7696\n",
      "Epoch 238/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7696 - val_loss: 2.7693\n",
      "Epoch 239/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7704 - val_loss: 2.7692\n",
      "Epoch 240/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7696 - val_loss: 2.7695\n",
      "Epoch 241/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7692 - val_loss: 2.7695\n",
      "Epoch 242/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7689 - val_loss: 2.7695\n",
      "Epoch 243/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7695 - val_loss: 2.7697\n",
      "Epoch 244/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7693 - val_loss: 2.7691\n",
      "Epoch 245/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7697 - val_loss: 2.7696\n",
      "Epoch 246/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7702 - val_loss: 2.7695\n",
      "Epoch 247/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7694 - val_loss: 2.7695\n",
      "Epoch 248/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7688 - val_loss: 2.7693\n",
      "Epoch 249/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7672 - val_loss: 2.7693\n",
      "Epoch 250/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7700 - val_loss: 2.7692\n",
      "Epoch 251/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s - loss: 2.7688 - val_loss: 2.7693\n",
      "Epoch 252/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7686 - val_loss: 2.7694\n",
      "Epoch 253/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7687 - val_loss: 2.7696\n",
      "Epoch 254/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7689 - val_loss: 2.7698\n",
      "Epoch 255/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7693 - val_loss: 2.7699\n",
      "Epoch 256/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7674 - val_loss: 2.7699\n",
      "Epoch 257/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7692 - val_loss: 2.7695\n",
      "Epoch 258/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7700 - val_loss: 2.7697\n",
      "Epoch 259/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7691 - val_loss: 2.7699\n",
      "Epoch 260/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7697 - val_loss: 2.7695\n",
      "Epoch 261/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7699 - val_loss: 2.7696\n",
      "Epoch 262/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7683 - val_loss: 2.7698\n",
      "Epoch 263/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7696 - val_loss: 2.7699\n",
      "Epoch 264/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7688 - val_loss: 2.7698\n",
      "Epoch 265/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7685 - val_loss: 2.7697\n",
      "Epoch 266/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7695 - val_loss: 2.7693\n",
      "Epoch 267/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7688 - val_loss: 2.7694\n",
      "Epoch 268/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7687 - val_loss: 2.7696\n",
      "Epoch 269/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7697 - val_loss: 2.7694\n",
      "Epoch 270/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7690 - val_loss: 2.7691\n",
      "Epoch 271/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7690 - val_loss: 2.7700\n",
      "Epoch 272/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7702 - val_loss: 2.7699\n",
      "Epoch 273/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7698 - val_loss: 2.7696\n",
      "Epoch 274/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7688 - val_loss: 2.7696\n",
      "Epoch 275/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7688 - val_loss: 2.7695\n",
      "Epoch 276/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7696 - val_loss: 2.7694\n",
      "Epoch 277/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7692 - val_loss: 2.7698\n",
      "Epoch 278/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7687 - val_loss: 2.7697\n",
      "Epoch 279/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7697 - val_loss: 2.7698\n",
      "Epoch 280/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7692 - val_loss: 2.7703\n",
      "Epoch 281/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7699 - val_loss: 2.7699\n",
      "Epoch 282/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7689 - val_loss: 2.7697\n",
      "Epoch 283/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7694 - val_loss: 2.7697\n",
      "Epoch 284/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7683 - val_loss: 2.7698\n",
      "Epoch 285/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7697 - val_loss: 2.7699\n",
      "Epoch 286/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7700 - val_loss: 2.7703\n",
      "Epoch 287/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7692 - val_loss: 2.7701\n",
      "Epoch 288/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7700 - val_loss: 2.7703\n",
      "Epoch 289/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7696 - val_loss: 2.7704\n",
      "Epoch 290/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7697 - val_loss: 2.7702\n",
      "Epoch 291/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7695 - val_loss: 2.7695\n",
      "Epoch 292/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7677 - val_loss: 2.7696\n",
      "Epoch 293/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7687 - val_loss: 2.7697\n",
      "Epoch 294/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7693 - val_loss: 2.7698\n",
      "Epoch 295/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7692 - val_loss: 2.7702\n",
      "Epoch 296/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7699 - val_loss: 2.7696\n",
      "Epoch 297/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7690 - val_loss: 2.7698\n",
      "Epoch 298/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7685 - val_loss: 2.7703\n",
      "Epoch 299/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7691 - val_loss: 2.7699\n",
      "Epoch 300/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7693 - val_loss: 2.7698\n",
      "Epoch 301/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7690 - val_loss: 2.7700\n",
      "Epoch 302/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7690 - val_loss: 2.7699\n",
      "Epoch 303/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7694 - val_loss: 2.7696\n",
      "Epoch 304/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7694 - val_loss: 2.7697\n",
      "Epoch 305/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7694 - val_loss: 2.7701\n",
      "Epoch 306/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7687 - val_loss: 2.7696\n",
      "Epoch 307/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7690 - val_loss: 2.7697\n",
      "Epoch 308/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7701 - val_loss: 2.7696\n",
      "Epoch 309/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7689 - val_loss: 2.7694\n",
      "Epoch 310/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7696 - val_loss: 2.7694\n",
      "Epoch 311/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7692 - val_loss: 2.7698\n",
      "Epoch 312/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7699 - val_loss: 2.7699\n",
      "Epoch 313/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7696 - val_loss: 2.7696\n",
      "Epoch 314/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7692 - val_loss: 2.7690\n",
      "Epoch 315/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7686 - val_loss: 2.7694\n",
      "Epoch 316/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7687 - val_loss: 2.7700\n",
      "Epoch 317/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7691 - val_loss: 2.7693\n",
      "Epoch 318/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7681 - val_loss: 2.7702\n",
      "Epoch 319/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7697 - val_loss: 2.7698\n",
      "Epoch 320/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7699 - val_loss: 2.7695\n",
      "Epoch 321/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7689 - val_loss: 2.7693\n",
      "Epoch 322/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7697 - val_loss: 2.7698\n",
      "Epoch 323/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7689 - val_loss: 2.7698\n",
      "Epoch 324/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7677 - val_loss: 2.7700\n",
      "Epoch 325/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7688 - val_loss: 2.7693\n",
      "Epoch 326/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7690 - val_loss: 2.7698\n",
      "Epoch 327/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7702 - val_loss: 2.7704\n",
      "Epoch 328/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7693 - val_loss: 2.7698\n",
      "Epoch 329/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7693 - val_loss: 2.7698\n",
      "Epoch 330/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7692 - val_loss: 2.7701\n",
      "Epoch 331/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7705 - val_loss: 2.7709ss: 2.770\n",
      "Epoch 332/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7698 - val_loss: 2.7695\n",
      "Epoch 333/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7699 - val_loss: 2.7691\n",
      "Epoch 334/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s - loss: 2.7691 - val_loss: 2.7698\n",
      "Epoch 335/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7698 - val_loss: 2.7703\n",
      "Epoch 336/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7682 - val_loss: 2.7694\n",
      "Epoch 337/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7689 - val_loss: 2.7698\n",
      "Epoch 338/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7696 - val_loss: 2.7695\n",
      "Epoch 339/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7686 - val_loss: 2.7688\n",
      "Epoch 340/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7689 - val_loss: 2.7694\n",
      "Epoch 341/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7699 - val_loss: 2.7691\n",
      "Epoch 342/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7693 - val_loss: 2.7695\n",
      "Epoch 343/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7692 - val_loss: 2.7692\n",
      "Epoch 344/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7683 - val_loss: 2.7691\n",
      "Epoch 345/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7677 - val_loss: 2.7696\n",
      "Epoch 346/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7679 - val_loss: 2.7696\n",
      "Epoch 347/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7687 - val_loss: 2.7696\n",
      "Epoch 348/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7700 - val_loss: 2.7693\n",
      "Epoch 349/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7689 - val_loss: 2.7696\n",
      "Epoch 350/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7696 - val_loss: 2.7694\n",
      "Epoch 351/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7693 - val_loss: 2.7694\n",
      "Epoch 352/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7682 - val_loss: 2.7699\n",
      "Epoch 353/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7698 - val_loss: 2.7703\n",
      "Epoch 354/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7693 - val_loss: 2.7702\n",
      "Epoch 355/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7689 - val_loss: 2.7692\n",
      "Epoch 356/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7685 - val_loss: 2.7692\n",
      "Epoch 357/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7689 - val_loss: 2.7698\n",
      "Epoch 358/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7685 - val_loss: 2.7698\n",
      "Epoch 359/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7692 - val_loss: 2.7695\n",
      "Epoch 360/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7691 - val_loss: 2.7695\n",
      "Epoch 361/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7697 - val_loss: 2.7697\n",
      "Epoch 362/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7693 - val_loss: 2.7696\n",
      "Epoch 363/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7683 - val_loss: 2.7697\n",
      "Epoch 364/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7688 - val_loss: 2.7696\n",
      "Epoch 365/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7679 - val_loss: 2.7691\n",
      "Epoch 366/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7698 - val_loss: 2.7696\n",
      "Epoch 367/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7686 - val_loss: 2.7699\n",
      "Epoch 368/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7695 - val_loss: 2.7690\n",
      "Epoch 369/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7701 - val_loss: 2.7693\n",
      "Epoch 370/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7681 - val_loss: 2.7693\n",
      "Epoch 371/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7693 - val_loss: 2.7697\n",
      "Epoch 372/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7693 - val_loss: 2.7695\n",
      "Epoch 373/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7687 - val_loss: 2.7691\n",
      "Epoch 374/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7689 - val_loss: 2.7690\n",
      "Epoch 375/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7693 - val_loss: 2.7691\n",
      "Epoch 376/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7694 - val_loss: 2.7698\n",
      "Epoch 377/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7695 - val_loss: 2.7697\n",
      "Epoch 378/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7693 - val_loss: 2.7692\n",
      "Epoch 379/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7698 - val_loss: 2.7694\n",
      "Epoch 380/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7698 - val_loss: 2.7687\n",
      "Epoch 381/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7690 - val_loss: 2.7693\n",
      "Epoch 382/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7690 - val_loss: 2.7697\n",
      "Epoch 383/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7688 - val_loss: 2.7695\n",
      "Epoch 384/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7692 - val_loss: 2.7697\n",
      "Epoch 385/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7688 - val_loss: 2.7691\n",
      "Epoch 386/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7695 - val_loss: 2.7695\n",
      "Epoch 387/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7686 - val_loss: 2.7694\n",
      "Epoch 388/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7693 - val_loss: 2.7694\n",
      "Epoch 389/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7692 - val_loss: 2.7695\n",
      "Epoch 390/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7683 - val_loss: 2.7695\n",
      "Epoch 391/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7682 - val_loss: 2.7689\n",
      "Epoch 392/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7685 - val_loss: 2.7693\n",
      "Epoch 393/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7687 - val_loss: 2.7698\n",
      "Epoch 394/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7690 - val_loss: 2.7697\n",
      "Epoch 395/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7695 - val_loss: 2.7694\n",
      "Epoch 396/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7691 - val_loss: 2.7698\n",
      "Epoch 397/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7685 - val_loss: 2.7695\n",
      "Epoch 398/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7688 - val_loss: 2.7695\n",
      "Epoch 399/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7687 - val_loss: 2.7696\n",
      "Epoch 400/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7698 - val_loss: 2.7696\n",
      "Epoch 401/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7688 - val_loss: 2.7700\n",
      "Epoch 402/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7687 - val_loss: 2.7701\n",
      "Epoch 403/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7695 - val_loss: 2.7694\n",
      "Epoch 404/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7695 - val_loss: 2.7694\n",
      "Epoch 405/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7686 - val_loss: 2.7693\n",
      "Epoch 406/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7689 - val_loss: 2.7695\n",
      "Epoch 407/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7679 - val_loss: 2.7699\n",
      "Epoch 408/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7691 - val_loss: 2.7697\n",
      "Epoch 409/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7685 - val_loss: 2.7697\n",
      "Epoch 410/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7690 - val_loss: 2.7695\n",
      "Epoch 411/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7688 - val_loss: 2.7693\n",
      "Epoch 412/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7692 - val_loss: 2.7691\n",
      "Epoch 413/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7691 - val_loss: 2.7696\n",
      "Epoch 414/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7694 - val_loss: 2.7698\n",
      "Epoch 415/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7690 - val_loss: 2.7696\n",
      "Epoch 416/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7696 - val_loss: 2.7694\n",
      "Epoch 417/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s - loss: 2.7681 - val_loss: 2.7699\n",
      "Epoch 418/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7678 - val_loss: 2.7696\n",
      "Epoch 419/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7696 - val_loss: 2.7700\n",
      "Epoch 420/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7698 - val_loss: 2.7701\n",
      "Epoch 421/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7700 - val_loss: 2.7700\n",
      "Epoch 422/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7690 - val_loss: 2.7698\n",
      "Epoch 423/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7697 - val_loss: 2.7697\n",
      "Epoch 424/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7695 - val_loss: 2.7696\n",
      "Epoch 425/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7689 - val_loss: 2.7697\n",
      "Epoch 426/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7695 - val_loss: 2.7698\n",
      "Epoch 427/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7689 - val_loss: 2.7702\n",
      "Epoch 428/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7693 - val_loss: 2.7698\n",
      "Epoch 429/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7694 - val_loss: 2.7703\n",
      "Epoch 430/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7688 - val_loss: 2.7695\n",
      "Epoch 431/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7702 - val_loss: 2.7703\n",
      "Epoch 432/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7701 - val_loss: 2.7691\n",
      "Epoch 433/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7684 - val_loss: 2.7696\n",
      "Epoch 434/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7699 - val_loss: 2.7697\n",
      "Epoch 435/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7687 - val_loss: 2.7696\n",
      "Epoch 436/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7696 - val_loss: 2.7691\n",
      "Epoch 437/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7687 - val_loss: 2.7697\n",
      "Epoch 438/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7695 - val_loss: 2.7696\n",
      "Epoch 439/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7696 - val_loss: 2.7696\n",
      "Epoch 440/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7681 - val_loss: 2.7698\n",
      "Epoch 441/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7696 - val_loss: 2.7695\n",
      "Epoch 442/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7693 - val_loss: 2.7700\n",
      "Epoch 443/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7694 - val_loss: 2.7699\n",
      "Epoch 444/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7697 - val_loss: 2.7699\n",
      "Epoch 445/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7685 - val_loss: 2.7697\n",
      "Epoch 446/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7690 - val_loss: 2.7694\n",
      "Epoch 447/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7692 - val_loss: 2.7698\n",
      "Epoch 448/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7697 - val_loss: 2.7698\n",
      "Epoch 449/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7691 - val_loss: 2.7692\n",
      "Epoch 450/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7691 - val_loss: 2.7697\n",
      "Epoch 451/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7689 - val_loss: 2.7698\n",
      "Epoch 452/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7692 - val_loss: 2.7699\n",
      "Epoch 453/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7692 - val_loss: 2.7702\n",
      "Epoch 454/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7699 - val_loss: 2.7701\n",
      "Epoch 455/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7688 - val_loss: 2.7700\n",
      "Epoch 456/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7687 - val_loss: 2.7693\n",
      "Epoch 457/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7689 - val_loss: 2.7695\n",
      "Epoch 458/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7685 - val_loss: 2.7697\n",
      "Epoch 459/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7701 - val_loss: 2.7691\n",
      "Epoch 460/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7692 - val_loss: 2.7694\n",
      "Epoch 461/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7693 - val_loss: 2.7697\n",
      "Epoch 462/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7689 - val_loss: 2.7697\n",
      "Epoch 463/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7682 - val_loss: 2.7695\n",
      "Epoch 464/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7677 - val_loss: 2.7698\n",
      "Epoch 465/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7685 - val_loss: 2.7688\n",
      "Epoch 466/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7691 - val_loss: 2.7695\n",
      "Epoch 467/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7685 - val_loss: 2.7699\n",
      "Epoch 468/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7689 - val_loss: 2.7698\n",
      "Epoch 469/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7689 - val_loss: 2.7699\n",
      "Epoch 470/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7686 - val_loss: 2.7694\n",
      "Epoch 471/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7688 - val_loss: 2.7694\n",
      "Epoch 472/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7695 - val_loss: 2.7695\n",
      "Epoch 473/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7687 - val_loss: 2.7702\n",
      "Epoch 474/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7701 - val_loss: 2.7696\n",
      "Epoch 475/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7691 - val_loss: 2.7694\n",
      "Epoch 476/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7687 - val_loss: 2.7695\n",
      "Epoch 477/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7694 - val_loss: 2.7701\n",
      "Epoch 478/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7697 - val_loss: 2.7695\n",
      "Epoch 479/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7696 - val_loss: 2.7696\n",
      "Epoch 480/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7692 - val_loss: 2.7692\n",
      "Epoch 481/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7691 - val_loss: 2.7699\n",
      "Epoch 482/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7681 - val_loss: 2.7698\n",
      "Epoch 483/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7692 - val_loss: 2.7700\n",
      "Epoch 484/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7682 - val_loss: 2.7697\n",
      "Epoch 485/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7700 - val_loss: 2.7692\n",
      "Epoch 486/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7696 - val_loss: 2.7699\n",
      "Epoch 487/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7699 - val_loss: 2.7698\n",
      "Epoch 488/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7688 - val_loss: 2.7696\n",
      "Epoch 489/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7688 - val_loss: 2.7700\n",
      "Epoch 490/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7689 - val_loss: 2.7702\n",
      "Epoch 491/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7692 - val_loss: 2.7699\n",
      "Epoch 492/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7691 - val_loss: 2.7695\n",
      "Epoch 493/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7692 - val_loss: 2.7701\n",
      "Epoch 494/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7691 - val_loss: 2.7701\n",
      "Epoch 495/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7685 - val_loss: 2.7700\n",
      "Epoch 496/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7694 - val_loss: 2.7694\n",
      "Epoch 497/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7698 - val_loss: 2.7702\n",
      "Epoch 498/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7686 - val_loss: 2.7699\n",
      "Epoch 499/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7699 - val_loss: 2.7695\n",
      "Epoch 500/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s - loss: 2.7684 - val_loss: 2.7700\n",
      "Epoch 501/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7682 - val_loss: 2.7700\n",
      "Epoch 502/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7677 - val_loss: 2.7698\n",
      "Epoch 503/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7695 - val_loss: 2.7694\n",
      "Epoch 504/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7698 - val_loss: 2.7696\n",
      "Epoch 505/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7700 - val_loss: 2.7703\n",
      "Epoch 506/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7691 - val_loss: 2.7702\n",
      "Epoch 507/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7693 - val_loss: 2.7700\n",
      "Epoch 508/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7701 - val_loss: 2.7697\n",
      "Epoch 509/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7689 - val_loss: 2.7703\n",
      "Epoch 510/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7698 - val_loss: 2.7700\n",
      "Epoch 511/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7684 - val_loss: 2.7697\n",
      "Epoch 512/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7682 - val_loss: 2.7699\n",
      "Epoch 513/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7685 - val_loss: 2.7702\n",
      "Epoch 514/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7690 - val_loss: 2.7698\n",
      "Epoch 515/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7694 - val_loss: 2.7700\n",
      "Epoch 516/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7686 - val_loss: 2.7694\n",
      "Epoch 517/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7682 - val_loss: 2.7697\n",
      "Epoch 518/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7692 - val_loss: 2.7698\n",
      "Epoch 519/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7695 - val_loss: 2.7699\n",
      "Epoch 520/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7701 - val_loss: 2.7702\n",
      "Epoch 521/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7691 - val_loss: 2.7701\n",
      "Epoch 522/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7689 - val_loss: 2.7704\n",
      "Epoch 523/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7695 - val_loss: 2.7705\n",
      "Epoch 524/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7694 - val_loss: 2.7696\n",
      "Epoch 525/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7680 - val_loss: 2.7698\n",
      "Epoch 526/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7693 - val_loss: 2.7699\n",
      "Epoch 527/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7681 - val_loss: 2.7703\n",
      "Epoch 528/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7696 - val_loss: 2.7697\n",
      "Epoch 529/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7680 - val_loss: 2.7695\n",
      "Epoch 530/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7697 - val_loss: 2.7697\n",
      "Epoch 531/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7698 - val_loss: 2.7700\n",
      "Epoch 532/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7681 - val_loss: 2.7697\n",
      "Epoch 533/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7697 - val_loss: 2.7696\n",
      "Epoch 534/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7695 - val_loss: 2.7704\n",
      "Epoch 535/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7689 - val_loss: 2.7699\n",
      "Epoch 536/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7697 - val_loss: 2.7698\n",
      "Epoch 537/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7680 - val_loss: 2.7703\n",
      "Epoch 538/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7691 - val_loss: 2.7701\n",
      "Epoch 539/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7687 - val_loss: 2.7700\n",
      "Epoch 540/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7697 - val_loss: 2.7703\n",
      "Epoch 541/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7691 - val_loss: 2.7699\n",
      "Epoch 542/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7699 - val_loss: 2.7701\n",
      "Epoch 543/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7709 - val_loss: 2.7700\n",
      "Epoch 544/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7687 - val_loss: 2.7704\n",
      "Epoch 545/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7688 - val_loss: 2.7703\n",
      "Epoch 546/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7685 - val_loss: 2.7694\n",
      "Epoch 547/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7697 - val_loss: 2.7697\n",
      "Epoch 548/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7695 - val_loss: 2.7695\n",
      "Epoch 549/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7694 - val_loss: 2.7700\n",
      "Epoch 550/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7694 - val_loss: 2.7702\n",
      "Epoch 551/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7694 - val_loss: 2.7697\n",
      "Epoch 552/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7691 - val_loss: 2.7698\n",
      "Epoch 553/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7693 - val_loss: 2.7694\n",
      "Epoch 554/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7683 - val_loss: 2.7701\n",
      "Epoch 555/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7693 - val_loss: 2.7696\n",
      "Epoch 556/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7682 - val_loss: 2.7697\n",
      "Epoch 557/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7698 - val_loss: 2.7701\n",
      "Epoch 558/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7697 - val_loss: 2.7691\n",
      "Epoch 559/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7690 - val_loss: 2.7703\n",
      "Epoch 560/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7692 - val_loss: 2.7697\n",
      "Epoch 561/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7692 - val_loss: 2.7696\n",
      "Epoch 562/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7679 - val_loss: 2.7693\n",
      "Epoch 563/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7696 - val_loss: 2.7695\n",
      "Epoch 564/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7692 - val_loss: 2.7694\n",
      "Epoch 565/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7694 - val_loss: 2.7697\n",
      "Epoch 566/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7687 - val_loss: 2.7694\n",
      "Epoch 567/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7698 - val_loss: 2.7698\n",
      "Epoch 568/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7692 - val_loss: 2.7695\n",
      "Epoch 569/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7687 - val_loss: 2.7694\n",
      "Epoch 570/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7688 - val_loss: 2.7693\n",
      "Epoch 571/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7686 - val_loss: 2.7697\n",
      "Epoch 572/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7692 - val_loss: 2.7695\n",
      "Epoch 573/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7690 - val_loss: 2.7696\n",
      "Epoch 574/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7693 - val_loss: 2.7696\n",
      "Epoch 575/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7692 - val_loss: 2.7696\n",
      "Epoch 576/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7690 - val_loss: 2.7697\n",
      "Epoch 577/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7691 - val_loss: 2.7699\n",
      "Epoch 578/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7696 - val_loss: 2.7696\n",
      "Epoch 579/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7694 - val_loss: 2.7697\n",
      "Epoch 580/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7681 - val_loss: 2.7693\n",
      "Epoch 581/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7691 - val_loss: 2.7696\n",
      "Epoch 582/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7692 - val_loss: 2.7697\n",
      "Epoch 583/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s - loss: 2.7701 - val_loss: 2.7698\n",
      "Epoch 584/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7687 - val_loss: 2.7700\n",
      "Epoch 585/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7683 - val_loss: 2.7699\n",
      "Epoch 586/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7702 - val_loss: 2.7699\n",
      "Epoch 587/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7700 - val_loss: 2.7702\n",
      "Epoch 588/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7692 - val_loss: 2.7696\n",
      "Epoch 589/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7694 - val_loss: 2.7697\n",
      "Epoch 590/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7687 - val_loss: 2.7696\n",
      "Epoch 591/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7704 - val_loss: 2.7697\n",
      "Epoch 592/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7699 - val_loss: 2.7700\n",
      "Epoch 593/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7700 - val_loss: 2.7698\n",
      "Epoch 594/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7691 - val_loss: 2.7698\n",
      "Epoch 595/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7699 - val_loss: 2.7696\n",
      "Epoch 596/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7692 - val_loss: 2.7700\n",
      "Epoch 597/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7683 - val_loss: 2.7699\n",
      "Epoch 598/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7688 - val_loss: 2.7690\n",
      "Epoch 599/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7692 - val_loss: 2.7697\n",
      "Epoch 600/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7701 - val_loss: 2.7700\n",
      "Epoch 601/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7690 - val_loss: 2.7700\n",
      "Epoch 602/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7691 - val_loss: 2.7696\n",
      "Epoch 603/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7683 - val_loss: 2.7699\n",
      "Epoch 604/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7700 - val_loss: 2.7700\n",
      "Epoch 605/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7688 - val_loss: 2.7694\n",
      "Epoch 606/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7687 - val_loss: 2.7699\n",
      "Epoch 607/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7692 - val_loss: 2.7701\n",
      "Epoch 608/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7693 - val_loss: 2.7694\n",
      "Epoch 609/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7683 - val_loss: 2.7698\n",
      "Epoch 610/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7701 - val_loss: 2.7699\n",
      "Epoch 611/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7697 - val_loss: 2.7702\n",
      "Epoch 612/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7688 - val_loss: 2.7698\n",
      "Epoch 613/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7684 - val_loss: 2.7694\n",
      "Epoch 614/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7696 - val_loss: 2.7694\n",
      "Epoch 615/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7696 - val_loss: 2.7698\n",
      "Epoch 616/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7682 - val_loss: 2.7699\n",
      "Epoch 617/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7694 - val_loss: 2.7709\n",
      "Epoch 618/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7684 - val_loss: 2.7694\n",
      "Epoch 619/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7688 - val_loss: 2.7701\n",
      "Epoch 620/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7694 - val_loss: 2.7702\n",
      "Epoch 621/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7687 - val_loss: 2.7701\n",
      "Epoch 622/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7682 - val_loss: 2.7704\n",
      "Epoch 623/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7696 - val_loss: 2.7701\n",
      "Epoch 624/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7693 - val_loss: 2.7696\n",
      "Epoch 625/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7692 - val_loss: 2.7698\n",
      "Epoch 626/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7693 - val_loss: 2.7698\n",
      "Epoch 627/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7703 - val_loss: 2.7704\n",
      "Epoch 628/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7689 - val_loss: 2.7701\n",
      "Epoch 629/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7690 - val_loss: 2.7693\n",
      "Epoch 630/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7685 - val_loss: 2.7695\n",
      "Epoch 631/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7700 - val_loss: 2.7700\n",
      "Epoch 632/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7693 - val_loss: 2.7700\n",
      "Epoch 633/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7694 - val_loss: 2.7705\n",
      "Epoch 634/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7693 - val_loss: 2.7701\n",
      "Epoch 635/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7684 - val_loss: 2.7701\n",
      "Epoch 636/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7694 - val_loss: 2.7699\n",
      "Epoch 637/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7695 - val_loss: 2.7699\n",
      "Epoch 638/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7688 - val_loss: 2.7699\n",
      "Epoch 639/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7700 - val_loss: 2.7694\n",
      "Epoch 640/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7687 - val_loss: 2.7700\n",
      "Epoch 641/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7691 - val_loss: 2.7707\n",
      "Epoch 642/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7694 - val_loss: 2.7704\n",
      "Epoch 643/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7697 - val_loss: 2.7695\n",
      "Epoch 644/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7680 - val_loss: 2.7700\n",
      "Epoch 645/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7683 - val_loss: 2.7700\n",
      "Epoch 646/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7699 - val_loss: 2.7696\n",
      "Epoch 647/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7702 - val_loss: 2.7697\n",
      "Epoch 648/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7680 - val_loss: 2.7696\n",
      "Epoch 649/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7683 - val_loss: 2.7695\n",
      "Epoch 650/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7697 - val_loss: 2.7695\n",
      "Epoch 651/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7693 - val_loss: 2.7697\n",
      "Epoch 652/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7692 - val_loss: 2.7702\n",
      "Epoch 653/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7690 - val_loss: 2.7701\n",
      "Epoch 654/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7704 - val_loss: 2.7698\n",
      "Epoch 655/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7692 - val_loss: 2.7695\n",
      "Epoch 656/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7680 - val_loss: 2.7694\n",
      "Epoch 657/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7699 - val_loss: 2.7699\n",
      "Epoch 658/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7679 - val_loss: 2.7703\n",
      "Epoch 659/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7693 - val_loss: 2.7703\n",
      "Epoch 660/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7678 - val_loss: 2.7704\n",
      "Epoch 661/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7694 - val_loss: 2.7699\n",
      "Epoch 662/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7700 - val_loss: 2.7699\n",
      "Epoch 663/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7708 - val_loss: 2.7696\n",
      "Epoch 664/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7685 - val_loss: 2.7704\n",
      "Epoch 665/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7698 - val_loss: 2.7700\n",
      "Epoch 666/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s - loss: 2.7694 - val_loss: 2.7697\n",
      "Epoch 667/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7684 - val_loss: 2.7701\n",
      "Epoch 668/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7686 - val_loss: 2.7702\n",
      "Epoch 669/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7698 - val_loss: 2.7695\n",
      "Epoch 670/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7688 - val_loss: 2.7699\n",
      "Epoch 671/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7686 - val_loss: 2.7697\n",
      "Epoch 672/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7693 - val_loss: 2.7700\n",
      "Epoch 673/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7693 - val_loss: 2.7700\n",
      "Epoch 674/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7682 - val_loss: 2.7702\n",
      "Epoch 675/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7691 - val_loss: 2.7703\n",
      "Epoch 676/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7684 - val_loss: 2.7694\n",
      "Epoch 677/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7695 - val_loss: 2.7695\n",
      "Epoch 678/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7693 - val_loss: 2.7696\n",
      "Epoch 679/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7697 - val_loss: 2.7699\n",
      "Epoch 680/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7689 - val_loss: 2.7699\n",
      "Epoch 681/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7693 - val_loss: 2.7693\n",
      "Epoch 682/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7681 - val_loss: 2.7698\n",
      "Epoch 683/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7684 - val_loss: 2.7699\n",
      "Epoch 684/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7689 - val_loss: 2.7701\n",
      "Epoch 685/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7694 - val_loss: 2.7698\n",
      "Epoch 686/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7693 - val_loss: 2.7695\n",
      "Epoch 687/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7685 - val_loss: 2.7692\n",
      "Epoch 688/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7684 - val_loss: 2.7693\n",
      "Epoch 689/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7690 - val_loss: 2.7697\n",
      "Epoch 690/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7692 - val_loss: 2.7695\n",
      "Epoch 691/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7702 - val_loss: 2.7701\n",
      "Epoch 692/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7688 - val_loss: 2.7693\n",
      "Epoch 693/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7695 - val_loss: 2.7697\n",
      "Epoch 694/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7698 - val_loss: 2.7692\n",
      "Epoch 695/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7689 - val_loss: 2.7696\n",
      "Epoch 696/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7706 - val_loss: 2.7697\n",
      "Epoch 697/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7684 - val_loss: 2.7692\n",
      "Epoch 698/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7695 - val_loss: 2.7697\n",
      "Epoch 699/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7693 - val_loss: 2.7696\n",
      "Epoch 700/700\n",
      "10000/10000 [==============================] - 0s - loss: 2.7692 - val_loss: 2.7703\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7475bffe80>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(data, data,\n",
    "                epochs=700,\n",
    "                batch_size=850,\n",
    "                validation_data=(val_data, val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "#autoencoder.save('2_2_symbol_autoencoder_v_best.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#autoencoder_loaded = load_model('4_7_symbol_autoencoder_v_best.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder = Model(input_signal, encoded2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = Input(shape=(n_channel,))\n",
    "\n",
    "deco = autoencoder.layers[-2](encoded_input)\n",
    "deco = autoencoder.layers[-1](deco)\n",
    "# create the decoder model\n",
    "decoder = Model(encoded_input, deco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 5000\n",
    "test_label = np.random.randint(M,size=N)\n",
    "test_data = []\n",
    "\n",
    "for i in test_label:\n",
    "    temp = np.zeros(M)\n",
    "    temp[i] = 1\n",
    "    test_data.append(temp)\n",
    "    \n",
    "test_data = np.array(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 15\n"
     ]
    }
   ],
   "source": [
    "temp_test = 6\n",
    "print (test_data[temp_test][test_label[temp_test]],test_label[temp_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[[ 7.00924444  8.4808836   5.3249526  ...,  6.38384104  5.0617547\n",
      "   6.28501701]\n",
      " [ 7.00924492  8.4808836   5.32495165 ...,  6.38384056  5.0617547\n",
      "   6.28501701]\n",
      " [ 7.0092454   8.48088264  5.32495213 ...,  6.38384104  5.0617547\n",
      "   6.28501701]\n",
      " ..., \n",
      " [ 2.22688723  5.27301121  2.53556347 ...,  0.28781193  3.56332684\n",
      "   4.10871553]\n",
      " [ 2.22688723  5.27301121  2.53556347 ...,  0.28781191  3.56332707\n",
      "   4.10871553]\n",
      " [ 2.22688699  5.27301121  2.53556323 ...,  0.28781191  3.56332707\n",
      "   4.10871553]]\n"
     ]
    }
   ],
   "source": [
    "autoencoder\n",
    "print (test_data[0])\n",
    "print (encoder.predict(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def frange(x, y, jump):\n",
    "  while x < y:\n",
    "    yield x\n",
    "    x += jump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNR: -2 BER: 0.9062\n",
      "SNR: -1.5 BER: 0.9034\n",
      "SNR: -1.0 BER: 0.9022\n",
      "SNR: -0.5 BER: 0.907\n",
      "SNR: 0.0 BER: 0.9022\n",
      "SNR: 0.5 BER: 0.9032\n",
      "SNR: 1.0 BER: 0.898\n",
      "SNR: 1.5 BER: 0.8974\n",
      "SNR: 2.0 BER: 0.8954\n",
      "SNR: 2.5 BER: 0.9024\n",
      "SNR: 3.0 BER: 0.8966\n",
      "SNR: 3.5 BER: 0.8952\n",
      "SNR: 4.0 BER: 0.8976\n",
      "SNR: 4.5 BER: 0.898\n",
      "SNR: 5.0 BER: 0.8904\n",
      "SNR: 5.5 BER: 0.8986\n",
      "SNR: 6.0 BER: 0.8952\n",
      "SNR: 6.5 BER: 0.8984\n",
      "SNR: 7.0 BER: 0.8956\n",
      "SNR: 7.5 BER: 0.8908\n",
      "SNR: 8.0 BER: 0.896\n",
      "SNR: 8.5 BER: 0.8926\n",
      "SNR: 9.0 BER: 0.8936\n",
      "SNR: 9.5 BER: 0.8956\n",
      "SNR: 10.0 BER: 0.892\n"
     ]
    }
   ],
   "source": [
    "EbNodB_range = list(frange(-2,10.5,0.5))\n",
    "ber = [None]*len(EbNodB_range)\n",
    "for n in range(0,len(EbNodB_range)):\n",
    "    EbNo=10.0**(EbNodB_range[n]/10.0)\n",
    "    noise_std = np.sqrt(1/(2*R*EbNo))\n",
    "    noise_mean = 0\n",
    "    no_errors = 0\n",
    "    nn = N\n",
    "    noise = noise_std * np.random.randn(nn,n_channel)\n",
    "    encoded_signal = encoder.predict(test_data) \n",
    "    final_signal = encoded_signal + noise\n",
    "    pred_final_signal =  decoder.predict(final_signal)\n",
    "    pred_output = np.argmax(pred_final_signal,axis=1)\n",
    "    no_errors = (pred_output != test_label)\n",
    "    no_errors =  no_errors.astype(int).sum()\n",
    "    ber[n] = no_errors / nn \n",
    "    print ('SNR:',EbNodB_range[n],'BER:',ber[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x74724d10b8>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy import interpolate\n",
    "plt.plot(EbNodB_range, ber, 'bo',label='Autoencoder(2,2)')\n",
    "#tck = interpolate.splrep(EbNodB_range, ber, s=0)\n",
    "#xnew = np.arange(-2,8.8, 0.6)\n",
    "#ynew = interpolate.splev(xnew, tck, der=0)\n",
    "#plt.plot(xnew,ynew,'y')\n",
    "#plt.plot(list(EbNodB_range), ber_theory, 'ro-',label='BPSK BER')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('SNR Range')\n",
    "plt.ylabel('Block Error Rate')\n",
    "plt.grid()\n",
    "plt.legend(loc='upper right',ncol = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ8AAAEKCAYAAADNSVhkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSHD/H9XHd2p13wx5AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7472942c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.savefig('AutoEncoder_2_2_BER_matplotlib')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking : expected input_4 to have shape (None, 16) but got array with shape (1, 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-bc4232ca4b21>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[0;32m   1565\u001b[0m         x = _standardize_input_data(x, self._feed_input_names,\n\u001b[0;32m   1566\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1567\u001b[1;33m                                     check_batch_axis=False)\n\u001b[0m\u001b[0;32m   1568\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1569\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mH:\\anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    137\u001b[0m                             \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m                             \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m                             str(array.shape))\n\u001b[0m\u001b[0;32m    140\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking : expected input_4 to have shape (None, 16) but got array with shape (1, 4)"
     ]
    }
   ],
   "source": [
    "print (encoder.predict(np.expand_dims([0,0,0,1],axis=0)))\n",
    "print (encoder.predict(np.expand_dims([0,0,1,0],axis=0)))\n",
    "print (encoder.predict(np.expand_dims([0,1,0,0],axis=0)))\n",
    "print (encoder.predict(np.expand_dims([1,0,0,0],axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
