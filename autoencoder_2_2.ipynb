{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libs\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Input, Dense, GaussianNoise,Lambda\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for reproducing reslut\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M: 4 k: 2\n"
     ]
    }
   ],
   "source": [
    "# defining parameters\n",
    "M = 4\n",
    "k = np.log2(M)\n",
    "k = int(k)\n",
    "print ('M:',M,'k:',k)\n",
    "R = 1\n",
    "n_channel = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generating data of size N\n",
    "N = 10000\n",
    "label = np.random.randint(M,size=N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating one hot encoded vectors\n",
    "data = []\n",
    "for i in label:\n",
    "    temp = np.zeros(M)\n",
    "    temp[i] = 1\n",
    "    data.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 4)\n"
     ]
    }
   ],
   "source": [
    "data = np.array(data)\n",
    "print (data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [ 0.  1.  0.  0.]\n",
      "0 [ 1.  0.  0.  0.]\n",
      "1 [ 0.  1.  0.  0.]\n",
      "3 [ 0.  0.  0.  1.]\n",
      "1 [ 0.  1.  0.  0.]\n",
      "3 [ 0.  0.  0.  1.]\n",
      "0 [ 1.  0.  0.  0.]\n",
      "3 [ 0.  0.  0.  1.]\n",
      "0 [ 1.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "temp_check = [17,23,45,67,89,96,72,250,350]\n",
    "for i in temp_check:\n",
    "    print(label[i],data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def antirectifier(x):\n",
    "    y = x/K.l2_normalize(x,axis=0)\n",
    "    return y\n",
    "def antirectifier_output_shape(input_shape):\n",
    "    return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print (int(k/R))\n",
    "input_signal = Input(shape=(M,))\n",
    "encoded = Dense(M, activation='relu')(input_signal)\n",
    "encoded1 = Dense(n_channel, activation='linear')(encoded)\n",
    "encoded2 = Lambda(lambda x: (np.sqrt(n_channel)*x)/K.l2_normalize(x,axis=0))(encoded1)\n",
    "#encoded2 = BatchNormalization()(encoded1)\n",
    "EbNo_train =  7 # 5.01187 #  coverted 7 db of EbNo\n",
    "encoded3 = GaussianNoise(np.sqrt(1/(2*R*EbNo_train)))(encoded2)\n",
    "\n",
    "decoded = Dense(M, activation='relu')(encoded3)\n",
    "decoded1 = Dense(M, activation='softmax')(decoded)\n",
    "autoencoder = Model(input_signal, decoded1)\n",
    "adam = Adam(lr=0.001)\n",
    "autoencoder.compile(optimizer=adam, loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "lambda_2 (Lambda)            (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_2 (GaussianNo (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 4)                 20        \n",
      "=================================================================\n",
      "Total params: 80\n",
      "Trainable params: 80\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print (autoencoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_val = 1500\n",
    "val_label = np.random.randint(M,size=N_val)\n",
    "val_data = []\n",
    "for i in val_label:\n",
    "    temp = np.zeros(M)\n",
    "    temp[i] = 1\n",
    "    val_data.append(temp)\n",
    "val_data = np.array(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 1500 samples\n",
      "Epoch 1/300\n",
      "10000/10000 [==============================] - 1s - loss: 3.0394 - val_loss: 2.3591\n",
      "Epoch 2/300\n",
      "10000/10000 [==============================] - 0s - loss: 2.1149 - val_loss: 1.7971\n",
      "Epoch 3/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.6778 - val_loss: 1.5071\n",
      "Epoch 4/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.4653 - val_loss: 1.4013\n",
      "Epoch 5/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3992 - val_loss: 1.3880\n",
      "Epoch 6/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3901 - val_loss: 1.3894\n",
      "Epoch 7/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3898 - val_loss: 1.3893\n",
      "Epoch 8/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3884 - val_loss: 1.3894\n",
      "Epoch 9/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3887 - val_loss: 1.3894\n",
      "Epoch 10/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3890 - val_loss: 1.3900\n",
      "Epoch 11/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3883 - val_loss: 1.3896\n",
      "Epoch 12/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3892 - val_loss: 1.3893\n",
      "Epoch 13/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3887 - val_loss: 1.3891\n",
      "Epoch 14/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3880 - val_loss: 1.3895\n",
      "Epoch 15/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3888 - val_loss: 1.3894\n",
      "Epoch 16/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3886 - val_loss: 1.3891\n",
      "Epoch 17/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3879 - val_loss: 1.3890\n",
      "Epoch 18/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3887 - val_loss: 1.3888\n",
      "Epoch 19/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3870 - val_loss: 1.3886\n",
      "Epoch 20/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3887 - val_loss: 1.3889\n",
      "Epoch 21/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3878 - val_loss: 1.3885\n",
      "Epoch 22/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3876 - val_loss: 1.3883\n",
      "Epoch 23/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3881 - val_loss: 1.3892\n",
      "Epoch 24/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3876 - val_loss: 1.3881\n",
      "Epoch 25/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3870 - val_loss: 1.3882\n",
      "Epoch 26/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3874 - val_loss: 1.3884\n",
      "Epoch 27/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3874 - val_loss: 1.3883\n",
      "Epoch 28/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3876 - val_loss: 1.3881\n",
      "Epoch 29/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3884 - val_loss: 1.3889\n",
      "Epoch 30/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3873 - val_loss: 1.3884\n",
      "Epoch 31/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3876 - val_loss: 1.3880\n",
      "Epoch 32/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3867 - val_loss: 1.3880\n",
      "Epoch 33/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3877 - val_loss: 1.3880\n",
      "Epoch 34/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3873 - val_loss: 1.3881\n",
      "Epoch 35/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3873 - val_loss: 1.3878\n",
      "Epoch 36/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3870 - val_loss: 1.3877\n",
      "Epoch 37/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3865 - val_loss: 1.3876\n",
      "Epoch 38/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3872 - val_loss: 1.3875\n",
      "Epoch 39/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3866 - val_loss: 1.3873\n",
      "Epoch 40/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3867 - val_loss: 1.3872\n",
      "Epoch 41/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3865 - val_loss: 1.3872\n",
      "Epoch 42/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3866 - val_loss: 1.3871\n",
      "Epoch 43/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3869\n",
      "Epoch 44/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3864 - val_loss: 1.3868\n",
      "Epoch 45/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3868\n",
      "Epoch 46/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3867\n",
      "Epoch 47/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3866\n",
      "Epoch 48/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3873 - val_loss: 1.3868\n",
      "Epoch 49/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3867\n",
      "Epoch 50/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3867\n",
      "Epoch 51/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3866\n",
      "Epoch 52/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3866 - val_loss: 1.3865\n",
      "Epoch 53/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3866\n",
      "Epoch 54/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3864 - val_loss: 1.3865\n",
      "Epoch 55/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3864\n",
      "Epoch 56/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 57/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3864\n",
      "Epoch 58/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 59/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3864\n",
      "Epoch 60/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 61/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3865\n",
      "Epoch 62/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 63/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3863\n",
      "Epoch 64/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3865\n",
      "Epoch 65/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 66/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 67/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 68/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 69/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 70/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3863\n",
      "Epoch 71/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 72/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3864\n",
      "Epoch 73/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 74/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 75/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3864\n",
      "Epoch 76/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 77/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3864\n",
      "Epoch 78/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3865\n",
      "Epoch 79/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 80/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 81/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3865\n",
      "Epoch 82/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3863\n",
      "Epoch 83/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 84/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3864\n",
      "Epoch 85/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3865\n",
      "Epoch 86/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3864\n",
      "Epoch 87/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3864\n",
      "Epoch 88/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3864\n",
      "Epoch 89/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 90/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 91/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 92/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3864\n",
      "Epoch 93/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 94/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 95/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3865\n",
      "Epoch 96/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3864\n",
      "Epoch 97/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3863\n",
      "Epoch 98/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 99/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 100/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 101/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 102/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3864\n",
      "Epoch 103/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 104/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 105/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3864\n",
      "Epoch 106/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 107/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 108/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 109/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 110/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 111/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 112/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3865\n",
      "Epoch 113/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 114/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 115/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3862\n",
      "Epoch 116/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 117/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 118/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3863\n",
      "Epoch 119/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 120/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 121/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 122/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3863\n",
      "Epoch 123/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 124/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 125/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 126/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3864\n",
      "Epoch 127/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3865\n",
      "Epoch 128/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 129/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 130/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 131/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 132/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3862\n",
      "Epoch 133/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 134/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3865\n",
      "Epoch 135/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3865\n",
      "Epoch 136/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 137/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 138/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 139/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 140/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 141/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 142/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 143/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 144/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 145/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3865\n",
      "Epoch 146/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 147/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3862\n",
      "Epoch 148/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 149/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3865\n",
      "Epoch 150/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 151/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 152/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 153/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 154/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 155/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 156/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 157/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 158/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3865\n",
      "Epoch 159/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3865\n",
      "Epoch 160/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 161/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 162/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 163/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 164/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3862\n",
      "Epoch 165/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 166/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 167/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 168/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 169/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3865\n",
      "Epoch 170/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 171/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 172/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 173/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 174/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 175/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 176/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 177/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 178/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 179/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 180/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 181/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 182/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 183/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 184/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 185/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 186/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 187/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 188/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 189/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 190/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 191/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 192/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 193/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 194/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 195/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3863\n",
      "Epoch 196/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3865\n",
      "Epoch 197/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3864\n",
      "Epoch 198/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 199/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 200/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3865\n",
      "Epoch 201/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3860 - val_loss: 1.3864\n",
      "Epoch 202/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3865 - val_loss: 1.3864\n",
      "Epoch 203/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3865\n",
      "Epoch 204/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3865 - val_loss: 1.3864\n",
      "Epoch 205/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 206/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 207/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 208/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 209/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 210/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 211/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 212/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 213/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 214/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 215/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 216/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 217/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 218/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 219/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 220/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 221/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 222/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3865\n",
      "Epoch 223/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 224/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 225/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 226/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 227/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3862\n",
      "Epoch 228/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3864\n",
      "Epoch 229/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 230/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 231/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 232/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 233/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 234/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 235/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 236/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 237/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3865\n",
      "Epoch 238/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3863\n",
      "Epoch 239/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 240/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 241/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 242/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 243/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3864\n",
      "Epoch 244/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 245/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 246/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 247/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 248/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3862\n",
      "Epoch 249/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 250/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3864\n",
      "Epoch 251/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 252/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 253/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 254/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 255/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 256/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 257/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 258/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 259/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3863\n",
      "Epoch 260/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 261/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 262/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 263/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3866\n",
      "Epoch 264/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 265/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 266/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 267/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3865\n",
      "Epoch 268/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 269/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 270/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 271/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3862\n",
      "Epoch 272/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3863\n",
      "Epoch 273/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 274/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 275/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 276/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 277/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 278/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 279/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3865\n",
      "Epoch 280/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 281/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 282/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 283/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 284/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 285/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 286/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 287/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 288/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 289/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3864\n",
      "Epoch 290/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 291/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 292/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 293/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 294/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 295/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 296/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 297/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 298/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 299/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 300/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x4d4295d7b8>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(data, data,\n",
    "                epochs=300,\n",
    "                batch_size=250,\n",
    "                validation_data=(val_data, val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "#autoencoder.save('2_2_symbol_autoencoder_v_best.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#autoencoder_loaded = load_model('4_7_symbol_autoencoder_v_best.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder = Model(input_signal, encoded2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded_input = Input(shape=(n_channel,))\n",
    "\n",
    "deco = autoencoder.layers[-2](encoded_input)\n",
    "deco = autoencoder.layers[-1](deco)\n",
    "# create the decoder model\n",
    "decoder = Model(encoded_input, deco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 5000\n",
    "test_label = np.random.randint(M,size=N)\n",
    "test_data = []\n",
    "\n",
    "for i in test_label:\n",
    "    temp = np.zeros(M)\n",
    "    temp[i] = 1\n",
    "    test_data.append(temp)\n",
    "    \n",
    "test_data = np.array(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 3\n"
     ]
    }
   ],
   "source": [
    "temp_test = 6\n",
    "print (test_data[temp_test][test_label[temp_test]],test_label[temp_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  1.]\n",
      "[[ 5.06002855  2.93496633  1.85008192  1.70522022]\n",
      " [ 5.06002855  2.93496633  1.85008192  1.70522022]\n",
      " [ 5.06002855  2.93496633  1.85008192  1.7052201 ]\n",
      " ..., \n",
      " [ 0.93277603  1.71216846  0.83922154  0.54186893]\n",
      " [ 0.93277603  1.71216857  0.83922148  0.54186887]\n",
      " [ 0.93277603  1.71216857  0.83922148  0.54186887]]\n"
     ]
    }
   ],
   "source": [
    "autoencoder\n",
    "print (test_data[0])\n",
    "print (encoder.predict(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def frange(x, y, jump):\n",
    "  while x < y:\n",
    "    yield x\n",
    "    x += jump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNR: -2 BER: 0.7462\n",
      "SNR: -1.5 BER: 0.7416\n",
      "SNR: -1.0 BER: 0.748\n",
      "SNR: -0.5 BER: 0.7438\n",
      "SNR: 0.0 BER: 0.7434\n",
      "SNR: 0.5 BER: 0.7474\n",
      "SNR: 1.0 BER: 0.7452\n",
      "SNR: 1.5 BER: 0.7458\n",
      "SNR: 2.0 BER: 0.7498\n",
      "SNR: 2.5 BER: 0.7486\n",
      "SNR: 3.0 BER: 0.7502\n",
      "SNR: 3.5 BER: 0.7486\n",
      "SNR: 4.0 BER: 0.7482\n",
      "SNR: 4.5 BER: 0.7502\n",
      "SNR: 5.0 BER: 0.7494\n",
      "SNR: 5.5 BER: 0.7478\n",
      "SNR: 6.0 BER: 0.7458\n",
      "SNR: 6.5 BER: 0.7482\n",
      "SNR: 7.0 BER: 0.7502\n",
      "SNR: 7.5 BER: 0.7482\n",
      "SNR: 8.0 BER: 0.7552\n",
      "SNR: 8.5 BER: 0.7488\n",
      "SNR: 9.0 BER: 0.753\n",
      "SNR: 9.5 BER: 0.754\n",
      "SNR: 10.0 BER: 0.7514\n"
     ]
    }
   ],
   "source": [
    "EbNodB_range = list(frange(-2,10.5,0.5))\n",
    "ber = [None]*len(EbNodB_range)\n",
    "for n in range(0,len(EbNodB_range)):\n",
    "    EbNo=10.0**(EbNodB_range[n]/10.0)\n",
    "    noise_std = np.sqrt(1/(2*R*EbNo))\n",
    "    noise_mean = 0\n",
    "    no_errors = 0\n",
    "    nn = N\n",
    "    noise = noise_std * np.random.randn(nn,n_channel)\n",
    "    encoded_signal = encoder.predict(test_data) \n",
    "    final_signal = encoded_signal + noise\n",
    "    pred_final_signal =  decoder.predict(final_signal)\n",
    "    pred_output = np.argmax(pred_final_signal,axis=1)\n",
    "    no_errors = (pred_output != test_label)\n",
    "    no_errors =  no_errors.astype(int).sum()\n",
    "    ber[n] = no_errors / nn \n",
    "    print ('SNR:',EbNodB_range[n],'BER:',ber[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x4d3c63b860>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy import interpolate\n",
    "plt.plot(EbNodB_range, ber, 'bo',label='Autoencoder(2,2)')\n",
    "#tck = interpolate.splrep(EbNodB_range, ber, s=0)\n",
    "#xnew = np.arange(-2,8.8, 0.6)\n",
    "#ynew = interpolate.splev(xnew, tck, der=0)\n",
    "#plt.plot(xnew,ynew,'y')\n",
    "#plt.plot(list(EbNodB_range), ber_theory, 'ro-',label='BPSK BER')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('SNR Range')\n",
    "plt.ylabel('Block Error Rate')\n",
    "plt.grid()\n",
    "plt.legend(loc='upper right',ncol = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAEKCAYAAAAyx7/DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niEjM/H/6l0bVFajU6AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x4d43b697f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.savefig('AutoEncoder_2_2_BER_matplotlib')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.37955412  0.3126522   0.02497371  0.20963533]]\n",
      "[[ 1.92966247  0.43632078  0.31874013  0.5554285 ]]\n",
      "[[ 0.12159108  0.91176349  0.02575182  0.12146593]]\n",
      "[[ 0.44358423  0.26863465  0.59179062  0.2424037 ]]\n"
     ]
    }
   ],
   "source": [
    "print (encoder.predict(np.expand_dims([0,0,0,1],axis=0)))\n",
    "print (encoder.predict(np.expand_dims([0,0,1,0],axis=0)))\n",
    "print (encoder.predict(np.expand_dims([0,1,0,0],axis=0)))\n",
    "print (encoder.predict(np.expand_dims([1,0,0,0],axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
