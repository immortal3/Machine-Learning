{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libs\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Input, Dense, GaussianNoise,Lambda\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam,SGD\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for reproducing reslut\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M: 4 k: 2 n: 2\n"
     ]
    }
   ],
   "source": [
    "# defining parameters\n",
    "M = 4\n",
    "k = np.log2(M)\n",
    "k = int(k)\n",
    "R = 1\n",
    "n_channel = 2\n",
    "print ('M:',M,'k:',k,'n:',n_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generating data of size N\n",
    "N = 8000\n",
    "label = np.random.randint(M,size=N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating one hot encoded vectors\n",
    "data = []\n",
    "for i in label:\n",
    "    temp = np.zeros(M)\n",
    "    temp[i] = 1\n",
    "    data.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 4)\n"
     ]
    }
   ],
   "source": [
    "data = np.array(data)\n",
    "print (data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [ 0.  1.  0.  0.]\n",
      "0 [ 1.  0.  0.  0.]\n",
      "1 [ 0.  1.  0.  0.]\n",
      "3 [ 0.  0.  0.  1.]\n",
      "1 [ 0.  1.  0.  0.]\n",
      "3 [ 0.  0.  0.  1.]\n",
      "0 [ 1.  0.  0.  0.]\n",
      "3 [ 0.  0.  0.  1.]\n",
      "0 [ 1.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "temp_check = [17,23,45,67,89,96,72,250,350]\n",
    "for i in temp_check:\n",
    "    print(label[i],data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def antirectifier(x):\n",
    "    y = x/K.l2_normalize(x,axis=0)\n",
    "    return \n",
    "def antirectifier_output_shape(input_shape):\n",
    "    return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_signal = Input(shape=(M,))\n",
    "encoded = Dense(M, activation='relu')(input_signal)\n",
    "encoded1 = Dense(n_channel, activation='linear')(encoded)\n",
    "encoded2 = Lambda(lambda x: x/tf.norm(x))(encoded1)\n",
    "\n",
    "EbNo_train = 5.01187 #  coverted 7 db of EbNo\n",
    "encoded3 = GaussianNoise(np.sqrt(1/(2*R*EbNo_train)))(encoded2)\n",
    "\n",
    "decoded = Dense(M, activation='relu')(encoded3)\n",
    "decoded1 = Dense(M, activation='softmax')(decoded)\n",
    "autoencoder = Model(input_signal, decoded1)\n",
    "adam = Adam(lr=0.01)\n",
    "autoencoder.compile(optimizer=adam, loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_33 (InputLayer)        (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_107 (Dense)            (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_108 (Dense)            (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "lambda_28 (Lambda)           (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_27 (GaussianN (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_109 (Dense)            (None, 4)                 12        \n",
      "_________________________________________________________________\n",
      "dense_110 (Dense)            (None, 4)                 20        \n",
      "=================================================================\n",
      "Total params: 62\n",
      "Trainable params: 62\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print (autoencoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tbCallBack = keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=32, write_graph=True, write_grads=True, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3700     \n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3680     \n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3699     \n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3684     \n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3650     \n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3651     \n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3650     \n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3671     \n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3661     \n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3670     \n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3687     \n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3678     \n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3637     \n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3718     \n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3714     \n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3691     \n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3689     \n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3689     \n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3657     \n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3709     \n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3658     \n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3708     \n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3665     \n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3643     \n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3672     \n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3678     \n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3668     \n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3661     \n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3684     \n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3685     \n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3654     \n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3668     \n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3723     \n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3658     \n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3675     \n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3674     \n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3649     \n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3690     \n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3699     \n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3679     \n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3641     \n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3662     \n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3685     \n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3664     \n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3651     \n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3693     \n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3650     \n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3688     \n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3655     \n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3652     \n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3634     \n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3686     \n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3671     \n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3638     \n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3668     \n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3645     \n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3635     \n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3669     \n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3666     \n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3690     \n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3664     \n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3684     \n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3647     \n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3667     \n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3673     \n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3676     \n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3693     \n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3624     \n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3674     \n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3702     \n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3662     \n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3680     \n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3663     \n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3643     \n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3654     \n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3679     \n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3670     \n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3665     \n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3670     \n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3707     \n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3642     \n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3644     \n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3644     \n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3640     \n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3694     \n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3710     \n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3683     \n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3640     \n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3704     \n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3677     \n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3652     \n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3628     \n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3654     \n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3698     \n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3683     \n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3673     \n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3699     \n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3667     \n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3686     \n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 0s - loss: 1.3712     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb5bb9f3f60>"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(data, data,\n",
    "                epochs=100,\n",
    "                batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "#autoencoder.save('2_2_symbol_autoencoder_v_best.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#autoencoder_loaded = load_model('4_7_symbol_autoencoder_v_best.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder = Model(input_signal, encoded2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded_input = Input(shape=(n_channel,))\n",
    "\n",
    "deco = autoencoder.layers[-2](encoded_input)\n",
    "deco = autoencoder.layers[-1](deco)\n",
    "# create the decoder model\n",
    "decoder = Model(encoded_input, deco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 5000\n",
    "test_label = np.random.randint(M,size=N)\n",
    "test_data = []\n",
    "\n",
    "for i in test_label:\n",
    "    temp = np.zeros(M)\n",
    "    temp[i] = 1\n",
    "    test_data.append(temp)\n",
    "    \n",
    "test_data = np.array(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 1\n"
     ]
    }
   ],
   "source": [
    "temp_test = 6\n",
    "print (test_data[temp_test][test_label[temp_test]],test_label[temp_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.99308366  0.11740891]]\n",
      "[[ 0.99988073  0.01544695]]\n",
      "[[ 0.89591837  0.44421872]]\n",
      "[[-0.99092066 -0.13444807]]\n"
     ]
    }
   ],
   "source": [
    "print (encoder.predict(np.expand_dims([0,0,0,1],axis=0)))\n",
    "print (encoder.predict(np.expand_dims([0,0,1,0],axis=0)))\n",
    "print (encoder.predict(np.expand_dims([0,1,0,0],axis=0)))\n",
    "print (encoder.predict(np.expand_dims([1,0,0,0],axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def frange(x, y, jump):\n",
    "  while x < y:\n",
    "    yield x\n",
    "    x += jump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNR: -2 BER: 0.6306\n",
      "SNR: -1.5 BER: 0.6118\n",
      "SNR: -1.0 BER: 0.6062\n",
      "SNR: -0.5 BER: 0.6048\n",
      "SNR: 0.0 BER: 0.5958\n",
      "SNR: 0.5 BER: 0.5762\n",
      "SNR: 1.0 BER: 0.5802\n",
      "SNR: 1.5 BER: 0.558\n",
      "SNR: 2.0 BER: 0.5442\n",
      "SNR: 2.5 BER: 0.534\n",
      "SNR: 3.0 BER: 0.5206\n",
      "SNR: 3.5 BER: 0.5064\n",
      "SNR: 4.0 BER: 0.5008\n",
      "SNR: 4.5 BER: 0.4768\n",
      "SNR: 5.0 BER: 0.462\n",
      "SNR: 5.5 BER: 0.4412\n",
      "SNR: 6.0 BER: 0.4248\n",
      "SNR: 6.5 BER: 0.4138\n",
      "SNR: 7.0 BER: 0.3848\n",
      "SNR: 7.5 BER: 0.369\n",
      "SNR: 8.0 BER: 0.3244\n",
      "SNR: 8.5 BER: 0.311\n",
      "SNR: 9.0 BER: 0.3032\n",
      "SNR: 9.5 BER: 0.2724\n",
      "SNR: 10.0 BER: 0.2558\n"
     ]
    }
   ],
   "source": [
    "EbNodB_range = list(frange(-2,10.5,0.5))\n",
    "ber = [None]*len(EbNodB_range)\n",
    "for n in range(0,len(EbNodB_range)):\n",
    "    EbNo=10.0**(EbNodB_range[n]/10.0)\n",
    "    noise_std = np.sqrt(1/(2*R*EbNo))\n",
    "    noise_mean = 0\n",
    "    no_errors = 0\n",
    "    nn = N\n",
    "    noise = noise_std * np.random.randn(nn,n_channel)\n",
    "    encoded_signal = encoder.predict(test_data) \n",
    "    final_signal = encoded_signal + noise\n",
    "    pred_final_signal =  decoder.predict(final_signal)\n",
    "    pred_output = np.argmax(pred_final_signal,axis=1)\n",
    "    no_errors = (pred_output != test_label)\n",
    "    no_errors =  no_errors.astype(int).sum()\n",
    "    ber[n] = no_errors / nn \n",
    "    print ('SNR:',EbNodB_range[n],'BER:',ber[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fb5d27c7898>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy import interpolate\n",
    "plt.plot(EbNodB_range, ber, 'bo',label='Autoencoder(2,2)')\n",
    "#tck = interpolate.splrep(EbNodB_range, ber, s=0)\n",
    "#xnew = np.arange(-2,8.8, 0.6)\n",
    "#ynew = interpolate.splev(xnew, tck, der=0)\n",
    "#plt.plot(xnew,ynew,'y')\n",
    "#plt.plot(list(EbNodB_range), ber_theory, 'ro-',label='BPSK BER')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('SNR Range')\n",
    "plt.ylabel('Block Error Rate')\n",
    "plt.grid()\n",
    "plt.legend(loc='upper right',ncol = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.savefig('AutoEncoder_2_2_BER_matplotlib')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
