{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libs\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Input, Dense, GaussianNoise,Lambda\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for reproducing reslut\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M: 4 k: 2\n"
     ]
    }
   ],
   "source": [
    "# defining parameters\n",
    "M = 4\n",
    "k = np.log2(M)\n",
    "k = int(k)\n",
    "print ('M:',M,'k:',k)\n",
    "R = 1\n",
    "n_channel = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generating data of size N\n",
    "N = 10000\n",
    "label = np.random.randint(M,size=N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating one hot encoded vectors\n",
    "data = []\n",
    "for i in label:\n",
    "    temp = np.zeros(M)\n",
    "    temp[i] = 1\n",
    "    data.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 4)\n"
     ]
    }
   ],
   "source": [
    "data = np.array(data)\n",
    "print (data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [ 0.  1.  0.  0.]\n",
      "0 [ 1.  0.  0.  0.]\n",
      "1 [ 0.  1.  0.  0.]\n",
      "3 [ 0.  0.  0.  1.]\n",
      "1 [ 0.  1.  0.  0.]\n",
      "3 [ 0.  0.  0.  1.]\n",
      "0 [ 1.  0.  0.  0.]\n",
      "3 [ 0.  0.  0.  1.]\n",
      "0 [ 1.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "temp_check = [17,23,45,67,89,96,72,250,350]\n",
    "for i in temp_check:\n",
    "    print(label[i],data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def antirectifier(x):\n",
    "    y = x/K.l2_normalize(x,axis=0)\n",
    "    return y\n",
    "def antirectifier_output_shape(input_shape):\n",
    "    return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print (int(k/R))\n",
    "input_signal = Input(shape=(M,))\n",
    "encoded = Dense(M, activation='relu')(input_signal)\n",
    "encoded1 = Dense(n_channel, activation='linear')(encoded)\n",
    "encoded2 = Lambda(lambda x: (np.sqrt(n_channel)*x)/K.l2_normalize(x,axis=0))(encoded1)\n",
    "#encoded2 = BatchNormalization()(encoded1)\n",
    "EbNo_train = 5.01187 #  coverted 7 db of EbNo\n",
    "encoded3 = GaussianNoise(np.sqrt(1/(2*R*EbNo_train)))(encoded2)\n",
    "\n",
    "decoded = Dense(M, activation='relu')(encoded3)\n",
    "decoded1 = Dense(M, activation='softmax')(decoded)\n",
    "autoencoder = Model(input_signal, decoded1)\n",
    "adam = Adam(lr=0.001)\n",
    "autoencoder.compile(optimizer=adam, loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "lambda_2 (Lambda)            (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_2 (GaussianNo (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 4)                 12        \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 4)                 20        \n",
      "=================================================================\n",
      "Total params: 62\n",
      "Trainable params: 62\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print (autoencoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_val = 1500\n",
    "val_label = np.random.randint(M,size=N_val)\n",
    "val_data = []\n",
    "for i in val_label:\n",
    "    temp = np.zeros(M)\n",
    "    temp[i] = 1\n",
    "    val_data.append(temp)\n",
    "val_data = np.array(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 1500 samples\n",
      "Epoch 1/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3874 - val_loss: 1.3868\n",
      "Epoch 2/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3868\n",
      "Epoch 3/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3868\n",
      "Epoch 4/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3866\n",
      "Epoch 5/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3865\n",
      "Epoch 6/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3865ss: 1.386\n",
      "Epoch 7/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3864\n",
      "Epoch 8/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 9/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 10/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3863\n",
      "Epoch 11/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 12/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 13/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3864\n",
      "Epoch 14/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 15/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3863\n",
      "Epoch 16/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 17/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 18/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 19/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 20/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 21/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 22/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 23/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 24/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 25/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 26/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 27/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 28/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 29/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 30/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 31/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 32/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 33/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3863\n",
      "Epoch 34/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 35/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 36/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3862\n",
      "Epoch 37/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3865\n",
      "Epoch 38/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 39/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 40/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 41/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 42/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3864\n",
      "Epoch 43/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 44/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 45/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 46/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 47/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3862\n",
      "Epoch 48/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3865\n",
      "Epoch 49/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3864\n",
      "Epoch 50/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3865\n",
      "Epoch 51/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 52/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 53/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 54/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 55/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 56/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 57/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 58/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 59/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 60/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 61/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 62/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3863\n",
      "Epoch 63/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3863\n",
      "Epoch 64/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3865\n",
      "Epoch 65/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3864\n",
      "Epoch 66/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 67/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 68/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 69/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 70/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 71/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 72/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3864\n",
      "Epoch 73/300\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 1.386 - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 74/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 75/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 76/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 77/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 78/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3865\n",
      "Epoch 79/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 80/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 81/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3865\n",
      "Epoch 82/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3862\n",
      "Epoch 83/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 84/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 85/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 86/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 87/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 88/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 89/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 90/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 91/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 92/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 93/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 94/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 95/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3865\n",
      "Epoch 96/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 97/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 98/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 99/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 100/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 101/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 102/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 103/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 104/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 105/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 106/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 107/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 108/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 109/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 110/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 111/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 112/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 113/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 114/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3864\n",
      "Epoch 115/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3862\n",
      "Epoch 116/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 117/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 118/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 119/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 120/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 121/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 122/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3862\n",
      "Epoch 123/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 124/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 125/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 126/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 127/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3865\n",
      "Epoch 128/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 129/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 130/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 131/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 132/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3862\n",
      "Epoch 133/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 134/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3865\n",
      "Epoch 135/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3865\n",
      "Epoch 136/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 137/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 138/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 139/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 140/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 141/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 142/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 143/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 144/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 145/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3865\n",
      "Epoch 146/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 147/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3862\n",
      "Epoch 148/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 149/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3865\n",
      "Epoch 150/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 151/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 152/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 153/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 154/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 155/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 156/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 157/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 158/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3865\n",
      "Epoch 159/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3865\n",
      "Epoch 160/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 161/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 162/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 163/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 164/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3862\n",
      "Epoch 165/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 166/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 167/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 168/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 169/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3865\n",
      "Epoch 170/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 171/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 172/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 173/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 174/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 175/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 176/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 177/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 178/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 179/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 180/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 181/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 182/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 183/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 184/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 185/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 186/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 187/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 188/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 189/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 190/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 191/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 192/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 193/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 194/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 195/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3863\n",
      "Epoch 196/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3865\n",
      "Epoch 197/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 198/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 199/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 200/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3865\n",
      "Epoch 201/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 202/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 203/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3865\n",
      "Epoch 204/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 205/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 206/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 207/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 208/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 209/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 210/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 211/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 212/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 213/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 214/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 215/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 216/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 217/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 218/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 219/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 220/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 221/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 222/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3865\n",
      "Epoch 223/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 224/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 225/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 226/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 227/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3862\n",
      "Epoch 228/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3864\n",
      "Epoch 229/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 230/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 231/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 232/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 233/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 234/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 235/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 236/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 237/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3865\n",
      "Epoch 238/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3863\n",
      "Epoch 239/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 240/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 241/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 242/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 243/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3864\n",
      "Epoch 244/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 245/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 246/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 247/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 248/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3862\n",
      "Epoch 249/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 250/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3864\n",
      "Epoch 251/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 252/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 253/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 254/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 255/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 256/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 257/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 258/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 259/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3863\n",
      "Epoch 260/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 261/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 262/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 263/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3866\n",
      "Epoch 264/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 265/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 266/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 267/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3865\n",
      "Epoch 268/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 269/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 270/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 271/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3862\n",
      "Epoch 272/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3863\n",
      "Epoch 273/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 274/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 275/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 276/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 277/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 278/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 279/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3865\n",
      "Epoch 280/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 281/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 282/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 283/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 284/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 285/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 286/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 287/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 288/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 289/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3863 - val_loss: 1.3864\n",
      "Epoch 290/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 291/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 292/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 293/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 294/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 295/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 296/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3864\n",
      "Epoch 297/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 298/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 299/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n",
      "Epoch 300/300\n",
      "10000/10000 [==============================] - 0s - loss: 1.3862 - val_loss: 1.3863\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xc675eaa58>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(data, data,\n",
    "                epochs=300,\n",
    "                batch_size=250,\n",
    "                validation_data=(val_data, val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "#autoencoder.save('2_2_symbol_autoencoder_v_best.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#autoencoder_loaded = load_model('4_7_symbol_autoencoder_v_best.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder = Model(input_signal, encoded2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded_input = Input(shape=(n_channel,))\n",
    "\n",
    "deco = autoencoder.layers[-2](encoded_input)\n",
    "deco = autoencoder.layers[-1](deco)\n",
    "# create the decoder model\n",
    "decoder = Model(encoded_input, deco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 5000\n",
    "test_label = np.random.randint(M,size=N)\n",
    "test_data = []\n",
    "\n",
    "for i in test_label:\n",
    "    temp = np.zeros(M)\n",
    "    temp[i] = 1\n",
    "    test_data.append(temp)\n",
    "    \n",
    "test_data = np.array(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 3\n"
     ]
    }
   ],
   "source": [
    "temp_test = 6\n",
    "print (test_data[temp_test][test_label[temp_test]],test_label[temp_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  1.]\n",
      "[[ 3.49272895  2.03690672]\n",
      " [ 3.49272895  2.03690672]\n",
      " [ 3.49272895  2.03690696]\n",
      " ..., \n",
      " [ 1.60212743  1.04907858]\n",
      " [ 1.60212743  1.04907858]\n",
      " [ 1.60212743  1.04907858]]\n"
     ]
    }
   ],
   "source": [
    "autoencoder\n",
    "print (test_data[0])\n",
    "print (encoder.predict(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def frange(x, y, jump):\n",
    "  while x < y:\n",
    "    yield x\n",
    "    x += jump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNR: -2 BER: 0.756\n",
      "SNR: -1.5 BER: 0.7486\n",
      "SNR: -1.0 BER: 0.7552\n",
      "SNR: -0.5 BER: 0.7484\n",
      "SNR: 0.0 BER: 0.7558\n",
      "SNR: 0.5 BER: 0.7524\n",
      "SNR: 1.0 BER: 0.7562\n",
      "SNR: 1.5 BER: 0.7532\n",
      "SNR: 2.0 BER: 0.7536\n",
      "SNR: 2.5 BER: 0.7544\n",
      "SNR: 3.0 BER: 0.7636\n",
      "SNR: 3.5 BER: 0.761\n",
      "SNR: 4.0 BER: 0.7552\n",
      "SNR: 4.5 BER: 0.7588\n",
      "SNR: 5.0 BER: 0.7578\n",
      "SNR: 5.5 BER: 0.7618\n",
      "SNR: 6.0 BER: 0.7614\n",
      "SNR: 6.5 BER: 0.7558\n",
      "SNR: 7.0 BER: 0.7588\n",
      "SNR: 7.5 BER: 0.7602\n",
      "SNR: 8.0 BER: 0.7578\n",
      "SNR: 8.5 BER: 0.7598\n",
      "SNR: 9.0 BER: 0.7596\n",
      "SNR: 9.5 BER: 0.7602\n",
      "SNR: 10.0 BER: 0.7604\n"
     ]
    }
   ],
   "source": [
    "EbNodB_range = list(frange(-2,10.5,0.5))\n",
    "ber = [None]*len(EbNodB_range)\n",
    "for n in range(0,len(EbNodB_range)):\n",
    "    EbNo=10.0**(EbNodB_range[n]/10.0)\n",
    "    noise_std = np.sqrt(1/(2*R*EbNo))\n",
    "    noise_mean = 0\n",
    "    no_errors = 0\n",
    "    nn = N\n",
    "    noise = noise_std * np.random.randn(nn,n_channel)\n",
    "    encoded_signal = encoder.predict(test_data) \n",
    "    final_signal = encoded_signal + noise\n",
    "    pred_final_signal =  decoder.predict(final_signal)\n",
    "    pred_output = np.argmax(pred_final_signal,axis=1)\n",
    "    no_errors = (pred_output != test_label)\n",
    "    no_errors =  no_errors.astype(int).sum()\n",
    "    ber[n] = no_errors / nn \n",
    "    print ('SNR:',EbNodB_range[n],'BER:',ber[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0xc67091ba8>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy import interpolate\n",
    "plt.plot(EbNodB_range, ber, 'bo',label='Autoencoder(2,2)')\n",
    "#tck = interpolate.splrep(EbNodB_range, ber, s=0)\n",
    "#xnew = np.arange(-2,8.8, 0.6)\n",
    "#ynew = interpolate.splev(xnew, tck, der=0)\n",
    "#plt.plot(xnew,ynew,'y')\n",
    "#plt.plot(list(EbNodB_range), ber_theory, 'ro-',label='BPSK BER')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('SNR Range')\n",
    "plt.ylabel('Block Error Rate')\n",
    "plt.grid()\n",
    "plt.legend(loc='upper right',ncol = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAEKCAYAAAAyx7/DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJoO3FAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc63fe5fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.savefig('AutoEncoder_2_2_BER_matplotlib')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.25207403  0.03499058]]\n",
      "[[ 0.99079341  0.45972362]]\n",
      "[[ 0.88256234  0.55138433]]\n",
      "[[ 0.14042699  0.30398908]]\n"
     ]
    }
   ],
   "source": [
    "print (encoder.predict(np.expand_dims([0,0,0,1],axis=0)))\n",
    "print (encoder.predict(np.expand_dims([0,0,1,0],axis=0)))\n",
    "print (encoder.predict(np.expand_dims([0,1,0,0],axis=0)))\n",
    "print (encoder.predict(np.expand_dims([1,0,0,0],axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
